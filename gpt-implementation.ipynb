{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:09:16.147650Z","iopub.execute_input":"2025-08-28T19:09:16.147975Z","iopub.status.idle":"2025-08-28T19:09:16.151603Z","shell.execute_reply.started":"2025-08-28T19:09:16.147954Z","shell.execute_reply":"2025-08-28T19:09:16.151000Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"Cropinky/rap_lyrics_english\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:09:16.155141Z","iopub.execute_input":"2025-08-28T19:09:16.155358Z","iopub.status.idle":"2025-08-28T19:09:16.881395Z","shell.execute_reply.started":"2025-08-28T19:09:16.155343Z","shell.execute_reply":"2025-08-28T19:09:16.880674Z"}},"outputs":[{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/47 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c30af3787b044a66a71563581a96578f"}},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"lyrics_only = [ds['train'][i]['text'] for i in range(38, len(ds['train']))]\n\ntext = \"\\n\".join(lyrics_only)\n\nwith open('input.txt', 'w', encoding='utf-8') as f:\n    f.write(text)\n\nprint(\"Corpus length:\", len(text))\nprint(\"\\nSample snippet:\\n\", text[:1000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:09:16.882941Z","iopub.execute_input":"2025-08-28T19:09:16.883388Z","iopub.status.idle":"2025-08-28T19:09:55.168621Z","shell.execute_reply.started":"2025-08-28T19:09:16.883371Z","shell.execute_reply":"2025-08-28T19:09:55.167955Z"}},"outputs":[{"name":"stdout","text":"Corpus length: 44090977\n\nSample snippet:\n Foxy Brown\n<BOS>\nMy Beyoncé[Chorus: Lil Durk]\nOoh, I like the way she move\nShorty my baby, my everything, she the truth\nTogether we cool, me and her can't lose\nKeep 'em on their feet, baby, I know they so confused\nShorty my Beyoncé\nDurk and DeJ, Durk and DeJ, Durk and DeJ\nShorty my Beyoncé\nDurk and DeJ, Durk and DeJ, Durk and DeJ\nMy Beyoncé\n\n[Verse 1: Lil Durk]\nTrippin' on that drank, but I know she worth it\nIndependent baby, I know she workin'\nAdriana's serving drinks, 20 bottles, urgent\nI know it can be better but nobody's perfect\nWe flirted for a minute, DeJ, that's my baby\nI ain't trippin', I'm like Henny, yeah I'm in her kidneys\nShe like to play her songs to the way I'm hittin' it\nTurn around like, \"Damn Durk, I like the way you hittin' it\"\nDon't believe the rumors, girl\nYou know I'll do you, girl\nI don't wanna hear the shit about the niggas\nThat tried to do you, girl\nFuck the past right now\nShawty got you right now\nAnd you hot right now\nYou can get it right now, baby\n[Chorus: Lil\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"# unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:09:55.169352Z","iopub.execute_input":"2025-08-28T19:09:55.169559Z","iopub.status.idle":"2025-08-28T19:09:55.526886Z","shell.execute_reply.started":"2025-08-28T19:09:55.169543Z","shell.execute_reply":"2025-08-28T19:09:55.526170Z"}},"outputs":[{"name":"stdout","text":"\t\n !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¡¦¨©®³´¹½¿ÀÁÃÅÇÈÉÎÑÖØàáâãäåæçèéêëíîïñòóôöøùúûüāćēğİıōŐœŞşūŽƆɔɛ˜ВГДЕИКМНПРСТЭЯабвгдежзийклмнопрстуфхцчшщыьэюяё،؟آابتثجحخدذرزسشصضطظعغفقلمنهوًَُِپچکگیḥ  ​‌‍‎‒–—‘’‚“”•… ‪‬ ′ ⁠₂€↗☆☣♂♡✞✧✰了你准吗备好抦️﻿𝐋𝐒𝐜𝐞𝐟𝐢𝐦𝐧𝐨𝐩𝐫𝐬𝐭𝐲𝗟𝗦𝗰𝗲𝗳𝗶𝗺𝗻𝗼𝗽𝗿𝘀𝘁𝘆🎤🐉🐭💯🔥😂😭🤔🤷\n329\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# training setup\nbatch_size = 16        # number of sequences processed in parallel\nblock_size = 32        # max sequence length considered as context\nmax_iters = 5000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64            # embedding dimension\nn_head = 4             # number of attention heads\nn_layer = 4            # number of transformer layers\ndropout = 0.0\n\ntorch.manual_seed(1337)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:09:55.528582Z","iopub.execute_input":"2025-08-28T19:09:55.528801Z","iopub.status.idle":"2025-08-28T19:09:55.541996Z","shell.execute_reply.started":"2025-08-28T19:09:55.528780Z","shell.execute_reply":"2025-08-28T19:09:55.541371Z"}},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x79a1bdfa74f0>"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"# load dataset\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# vocabulary construction\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]    # string -> int list\ndecode = lambda l: ''.join([itos[i] for i in l])   # int list -> string\n\n# data split\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:09:55.542870Z","iopub.execute_input":"2025-08-28T19:09:55.543117Z","iopub.status.idle":"2025-08-28T19:10:01.325009Z","shell.execute_reply.started":"2025-08-28T19:09:55.543099Z","shell.execute_reply":"2025-08-28T19:10:01.324369Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# get random mini-batch\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x.to(device), y.to(device)\n\n# evaluation loop (average loss on train/val)\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            _, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:10:01.325657Z","iopub.execute_input":"2025-08-28T19:10:01.325862Z","iopub.status.idle":"2025-08-28T19:10:01.331588Z","shell.execute_reply.started":"2025-08-28T19:10:01.325846Z","shell.execute_reply":"2025-08-28T19:10:01.330844Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# single self-attention head\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * C**-0.5\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\n# multi-head attention block\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\n# feedforward network (MLP style)\nclass FeedFoward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# transformer block: attention + feedforward\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:10:01.332266Z","iopub.execute_input":"2025-08-28T19:10:01.332488Z","iopub.status.idle":"2025-08-28T19:10:01.535719Z","shell.execute_reply.started":"2025-08-28T19:10:01.332466Z","shell.execute_reply":"2025-08-28T19:10:01.535161Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# main language model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    # text generation from a starting prompt\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:10:01.536482Z","iopub.execute_input":"2025-08-28T19:10:01.536692Z","iopub.status.idle":"2025-08-28T19:10:01.554083Z","shell.execute_reply.started":"2025-08-28T19:10:01.536676Z","shell.execute_reply":"2025-08-28T19:10:01.553413Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# initialize model\nmodel = BigramLanguageModel()\nm = model.to(device)\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n# training loop\nfor iter in range(max_iters):\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    xb, yb = get_batch('train')\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:10:01.554690Z","iopub.execute_input":"2025-08-28T19:10:01.554879Z","iopub.status.idle":"2025-08-28T19:13:58.713840Z","shell.execute_reply.started":"2025-08-28T19:10:01.554865Z","shell.execute_reply":"2025-08-28T19:13:58.713162Z"}},"outputs":[{"name":"stdout","text":"0.243785 M parameters\nstep 0: train loss 5.9259, val loss 5.9287\nstep 100: train loss 2.8646, val loss 2.8600\nstep 200: train loss 2.6435, val loss 2.6438\nstep 300: train loss 2.5413, val loss 2.5407\nstep 400: train loss 2.4569, val loss 2.4777\nstep 500: train loss 2.3738, val loss 2.3874\nstep 600: train loss 2.3268, val loss 2.3420\nstep 700: train loss 2.2551, val loss 2.2878\nstep 800: train loss 2.2169, val loss 2.2435\nstep 900: train loss 2.1815, val loss 2.2185\nstep 1000: train loss 2.1322, val loss 2.1833\nstep 1100: train loss 2.1112, val loss 2.1498\nstep 1200: train loss 2.0923, val loss 2.1075\nstep 1300: train loss 2.0617, val loss 2.0808\nstep 1400: train loss 2.0488, val loss 2.0660\nstep 1500: train loss 2.0191, val loss 2.0509\nstep 1600: train loss 1.9975, val loss 2.0514\nstep 1700: train loss 1.9901, val loss 2.0264\nstep 1800: train loss 1.9669, val loss 2.0089\nstep 1900: train loss 1.9356, val loss 1.9972\nstep 2000: train loss 1.9476, val loss 1.9933\nstep 2100: train loss 1.9333, val loss 1.9808\nstep 2200: train loss 1.9255, val loss 1.9543\nstep 2300: train loss 1.9082, val loss 1.9512\nstep 2400: train loss 1.9114, val loss 1.9423\nstep 2500: train loss 1.8799, val loss 1.9333\nstep 2600: train loss 1.8932, val loss 1.9343\nstep 2700: train loss 1.8801, val loss 1.9257\nstep 2800: train loss 1.8885, val loss 1.9208\nstep 2900: train loss 1.8494, val loss 1.9085\nstep 3000: train loss 1.8610, val loss 1.9200\nstep 3100: train loss 1.8448, val loss 1.8962\nstep 3200: train loss 1.8291, val loss 1.8808\nstep 3300: train loss 1.8492, val loss 1.8859\nstep 3400: train loss 1.8304, val loss 1.8760\nstep 3500: train loss 1.8229, val loss 1.8769\nstep 3600: train loss 1.8206, val loss 1.8644\nstep 3700: train loss 1.8347, val loss 1.8612\nstep 3800: train loss 1.8048, val loss 1.8528\nstep 3900: train loss 1.8064, val loss 1.8650\nstep 4000: train loss 1.7986, val loss 1.8595\nstep 4100: train loss 1.7980, val loss 1.8529\nstep 4200: train loss 1.7930, val loss 1.8564\nstep 4300: train loss 1.7976, val loss 1.8481\nstep 4400: train loss 1.7902, val loss 1.8440\nstep 4500: train loss 1.7892, val loss 1.8354\nstep 4600: train loss 1.7980, val loss 1.8266\nstep 4700: train loss 1.7750, val loss 1.8278\nstep 4800: train loss 1.7851, val loss 1.8296\nstep 4900: train loss 1.7751, val loss 1.8082\nstep 4999: train loss 1.7667, val loss 1.8317\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:18:05.364593Z","iopub.execute_input":"2025-08-28T19:18:05.365411Z","iopub.status.idle":"2025-08-28T19:18:20.758497Z","shell.execute_reply.started":"2025-08-28T19:18:05.365378Z","shell.execute_reply":"2025-08-28T19:18:20.757732Z"}},"outputs":[{"name":"stdout","text":"\t, a might yeah, you'll very stupbo\nWe live up out pawed to\nJudn my desa niggas\n\nIf some the opping, you know they king you my ingeler moke Hold Ree, Willa\nHe good me, no sgal a he could in\nUh\nI drest teln me lot me! Lost, it muther?'t fuck you meming of alreens to got is fine\n? I’ma get avil the fallin', me sight night\nEvercida, do up, hompas rainive digings and\nGot one at rour\nI got got our shore, \"toes, we fain\n\n[Verse Skas]\n\n\n[Chorus: Losts]\nYou jegir my fifter to sould the pophed oughter\nI'm harry main is seer builly in twened stang two the crazz hop amo how easper did\nCopsin' waring on my yesou nopping with (Sheave, all fect the Cornives\n(How, pulac out rynal here moven\n\n[Verse 2: Mine Poppresed]\n. ceminger mean it we though!\n\nOh ya you hear the known, stuppin' are\nCome charger like {-SE L Wuirty, reace alah I some a gony\nSouthing tugas us\nHow your will ridge? (He's)]\nGrau flarging charp, lies splug that dick\nThisger withnould vound and the thatem pop\nThat from frees down som\nBecan icto plos, when yis\n'emings trong, yeah, yeah\nChoruldid is send you hear life\nBitch I rebrack wrip in the seeftie”\nMy, my can't, it's a pring drow my nigga red on Let's go, the mall fill, it up out\nMac wild a slitt the strain goting these nigga\nLift the Loof fliquess\n\n[Hook]\nWourh the know, fiver to boft, wake a frozy to makin'h-mozEmbedShare URLCopyEmbedCod****\n<AEmbody to ready new, Chorlin' Somorive yrifuck to drorin' life a casker\nShe moniamor,\nSome with track as on you done\nWe man have, you know whatfout 'em I'm aynitle\n\n[Reek Lilk]\nIt' doubdy iscone, yeah\nKnow me this these, me offings.\n\nEver orced back I fuck you got vied ups\nSwe a millst the gaggicts, Tight bacies\nI'm not in me face mine ya mot feels\nCorneing her worsi 5\nBecraded riend the swill dram is grintbled dad\nA Gotta hve cold to Take the me, C-Taisa There\n\n[Verse]\nNot in they A wild bitch, the ride?\n[Outhook: Stes]\nShown I got a busts\nThat conny up dimes eyes don't hee don't know\nYou know she fuckin, twe mompried\nI be\n","output_type":"stream"}],"execution_count":64}]}